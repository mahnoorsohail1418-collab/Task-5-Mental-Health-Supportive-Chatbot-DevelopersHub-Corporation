# ðŸ”¹ Full Supportive Chatbot with LoRA (Colab, safe max_new_tokens)
!pip install -q transformers datasets accelerate peft bitsandbytes

import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model, TaskType

# 1ï¸âƒ£ Build 200-example toy dataset
emotions_responses = {
    "stressed": [
        "It's normal to feel stressed. Try taking a short walk or listening to music.",
        "I hear you. Maybe writing down your thoughts can help clear your mind.",
        "Take a few deep breaths. Step by step, you can handle this."
    ],
    "anxious": [
        "It's okay to feel anxious. Focus on what you can control right now.",
        "Try to take a few deep breaths and center yourself.",
        "Remember, anxiety is temporary. You can get through it."
    ],
    "lonely": [
        "You are not alone. I'm here to listen.",
        "Talking to someone you trust can help you feel less lonely.",
        "It's okay to feel lonely sometimes. You matter."
    ],
    "sad": [
        "It's okay to feel sad. Let your emotions out and be kind to yourself.",
        "Sadness is natural. Talking about it can help you feel better.",
        "Take a moment to care for yourself. You're doing your best."
    ],
    "frustrated": [
        "It's normal to feel frustrated. Take a short break or breathe deeply.",
        "Try to pause and reflect before reacting. Youâ€™ll feel calmer.",
        "Feeling frustrated happens. Step back and take care of yourself."
    ],
    "guilty": [
        "Acknowledge it, learn, and forgive yourself. Everyone makes mistakes.",
        "Guilt can be heavy. Try to focus on how to move forward positively.",
        "It's okay. Be gentle with yourself and take small steps."
    ],
    "tired": [
        "Rest and self-care are important. Take a short nap or relax.",
        "Listen to your body. Give yourself permission to rest.",
        "Feeling tired is normal. A little break can help recharge your energy."
    ]
}

train_data = []
# Generate 200 examples
for i in range(200):
    for emotion, responses in emotions_responses.items():
        user_text = f"I feel {emotion} today."
        bot_text = responses[i % len(responses)]
        train_data.append({"text": f"User: {user_text}\nSupportiveBot: {bot_text}"})

# 2ï¸âƒ£ Convert to Hugging Face Dataset
train_dataset = Dataset.from_list(train_data)

# 3ï¸âƒ£ Load tokenizer & model
model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(model_name)

# 4ï¸âƒ£ Apply LoRA adapters
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["c_proj","q_proj","v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)

# 5ï¸âƒ£ Tokenize dataset
def tokenize_fn(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)

tokenized_dataset = train_dataset.map(tokenize_fn, batched=True, remove_columns=["text"])

# 6ï¸âƒ£ Data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# 7ï¸âƒ£ Training arguments
training_args = TrainingArguments(
    output_dir="./supportive-bot-lora",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=100,
    save_total_limit=2,
    logging_steps=20,
    learning_rate=3e-4,
    fp16=torch.cuda.is_available(),
    report_to="none"
)

# 8ï¸âƒ£ Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)

# 9ï¸âƒ£ Train LoRA adapters
trainer.train()
trainer.save_model("./supportive-bot-lora")
tokenizer.save_pretrained("./supportive-bot-lora")

# ðŸ”¹ Conversation history and safe generation
conversation_history = []

def generate_response(user_input, max_new_tokens=60):
    # Include last 2 turns
    history_text = ""
    if len(conversation_history) >= 2:
        history_text = "\n".join(conversation_history[-2:]) + "\n"

    prompt = f"You are a kind and supportive mental health assistant.\n{history_text}User: {user_input}\nSupportiveBot:"

    inputs = tokenizer(prompt, return_tensors="pt")
    output = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,      # safe for long inputs
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7,
        repetition_penalty=1.2,
        pad_token_id=tokenizer.eos_token_id
    )
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    response = decoded.split("SupportiveBot:")[-1].strip()

    conversation_history.append(f"User: {user_input}")
    conversation_history.append(f"SupportiveBot: {response}")

    return response

# ðŸ”¹ CLI Chat
print("ðŸ§  Supportive Chatbot (type 'exit' to quit)\n")
while True:
    user_input = input("You: ")
    if user_input.lower() == "exit":
        break
    response = generate_response(user_input)
    print("SupportiveBot:", response)
Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]
GPT2LMHeadModel LOAD REPORT from: distilgpt2
Key                                        | Status     |  | 
-------------------------------------------+------------+--+-
transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | 

Notes:
- UNEXPECTED	:can be ignored when loading from different task/architecture; not ok if you expect identical arch.
/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.
  warnings.warn(
Map:   0%|          | 0/1400 [00:00<?, ? examples/s]
/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
  warnings.warn(warn_msg)
/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
  warnings.warn(warn_msg)
/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
  warnings.warn(warn_msg)
ðŸ§  Supportive Chatbot (type 'exit' to quit)

You: i feel sad
SupportiveBot: It's okay to feel sad sometimes. You matter. Let your emotions out and be kind to yourself. You're doing your best. You can take care of yourself or relax. Try taking care that you care very little."
You: im about to cry
SupportiveBot: Take a few deep breaths and center yourself. You make good choices. Step by step, you'll handle this. Youâ€™ll feel better. Focus on how you can move forward positively. You make your best use between now and then! Everyone makes mistakes. Everyone takes responsibility back. Step by
You: im anxious
SupportiveBot: Try taking small steps or reacting quickly before reacting. You're not focusing too much anymore. You matter. It's normalâ€”if you can help you move forward fast and focus less. Your body is doing its best thing. Let your focus rest for itself."step-performedness has been greatly
You: exit